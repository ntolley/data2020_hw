---
title: "Assignment 3"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**NAME: Nicholas Tolley**\
**DUE DATE: March 21th, 6:00pm**

## Problem 1 (100 pts)

In the folder Assignment 3, you will find the data set called FF_wave6_2020v2.dta. This data set is from the Fragile Family Data Set, and it includes many different variables (socio-demographic, economics, and health status) of teenagers (15 years old) and their parents. The codebook (ff_wave6_codebook.txt) associated with the data set is on Canvas (folder Assignment 3).

Loading the dataset:

```{r}
rm(list=ls())
library(plyr)
library(readr)
library(dplyr)
library(glmnet)
library(haven)
library(tidyr)
library(caret)
library(boot)
library(MASS)
set.seed(1)
```

(a) (20 points) Consider the variable *doctor diagnosed youth with depression/anxiety*. In the data set, the name of this variable is *p6b5*. Then consider in the data set these variables: *p6b10*, *p6b35*, *p6b55*, *p6b60*, *p6c21*, *p6f32*, *p6f35*, *p6h74*, *p6h102*, *p6i7*, *p6i8*, *p6i11*, *p6j37*, *k6b21a*, *k6b22a*, *k6c1*, *k6c4e*, *k6c28*, *k6d37*, *k6f63*, *ck6cbmi*, *k6d10*. Now, you have a data set with 4898 subjects and 23 variables. Clean the data in these three steps. 1- Each variable has a value with a number and a text (for example, a value for the variable *p6b5* is *2 No*). Remove the text from all the variables in the data set (hint: use the function sub for each column). 2- Transform each variable in numeric (hint: use the function as.numeric for each column). 3- Transform all the values less than 0 in NA and then remove all your NA values from the data set. Show the dimensions of the cleaned data and print the first 6 rows.

The code below subsets the columns indicated, transforms the columns into type numeric, replaces negative numbers with NA, and then drops NA rows from the dataframe.

```{r}
subset_cols <- c('p6b5', 'p6b10', 'p6b35', 'p6b55', 'p6b60', 'p6c21', 'p6f32', 'p6f35', 'p6h74', 'p6h102', 'p6i7', 'p6i8', 'p6i11', 'p6j37', 'k6b21a', 'k6b22a', 'k6c1', 'k6c4e', 'k6c28', 'k6d37', 'k6f63', 'ck6cbmi', 'k6d10')

df<-read_dta(file='FF_wave6_2020v2.dta')
df <- subset(df, select=subset_cols)

for (col in subset_cols){
  df[col] <- as.numeric(unlist(df[col]))
}

df[df < 0] <- NA
df <- drop_na(df)



```

This produces a dataframe with dimensions:

```{r}
dim(df)
```

And the following first 6 rows:

```{r}
df[1:6,]
```

(a) (20 points) Now call the variables with an appropriate name (for example *p6b5* can become *Depression*). Perform a logistic regression using the variable *Depression* as the outcome and the remaining variables as the covariates. Be careful: the variable *Depression* has value 1 and 2, you should transform in 0,1 before running the logistic regression in R (1 for Yes, 0 for No). What are the important and significant covariates for the depression? For these, what can you say about the standard error? Perform the binned residual plot by using the library ggplot2 in R. Then write a function in R that gives the odds ratio for each beta and its upper and lower confidence intervals (CI). Use this function to produce the beta coefficient related to the covariate (ADD or *p6b10*), and its CI in term of ODDS RATIO. What can you say about that? Is still significant?

The code below renames the columns to more human readable names, as well as transforms the outcome column to values of 0 or 1

```{r}
col_names <- c('depression',	'ADD',	'mean',	'sleep_trouble',	'runaway',	'suspended',	'drug_problem',	'parent_jailed',	'smoker',	'jailed',	'neighbor_help',	'close_neighborhood',	'gang_problem',	'free_food',	'school_attention_problem',	'sports_team',	'parent_relationship',	'calm_home',	'father_close',	'physically_active',	'marijuana',	'BMI',	'menstruation_age')

df_pred <- df
colnames(df_pred) <- col_names
df_pred['depression'] <- df_pred['depression'] - 1

```

Next we fit a logistic regression model to the data and summarize the results. As shown, the significant covariates include: ADD, sleep_trouble, suspended, and school_attention_problem

The reason they are significant is because the values have a very low probability of being equal to zero (the null hypothesis), therefore the standard errors for the significant coefficients are relatively small compared to the magnitude of the coefficient's estimate.

```{r}
fit_logistic <- glm(depression ~ ., data=df_pred, family=binomial(link="logit"))
summary_logistic <- summary(fit_logistic)
summary_logistic
```

Next we can visualize the errors of this model using a binned residual plot

```{r}
pred_logistic <- fit_logistic$fitted.values

binned.resids <- function (x, y, nclass=sqrt(length(x))){
  breaks.index <- floor(length(x)*(1:(nclass-1))/nclass)
  breaks <- c (-Inf, sort(x)[breaks.index], Inf)
  output <- NULL
  xbreaks <- NULL
  x.binned <- as.numeric (cut (x, breaks))
  for (i in 1:nclass){
    items <- (1:length(x))[x.binned==i]
    x.range <- range(x[items])
    xbar <- mean(x[items])
    ybar <- mean(y[items])
    n <- length(items)
    sdev <- sd(y[items])
    output <- rbind (output, c(xbar, ybar, n, x.range, 2*sdev/sqrt(n)))
  }
  colnames (output) <- c ("xbar", "ybar", "n", "x.lo", "x.hi", "2se")
  return (list (binned=output, xbreaks=xbreaks))
}

br <- binned.resids(pred_logistic, df_pred$depression-pred_logistic, nclass=10)$binned
plot(range(br[,1]), range(br[,2],br[,6],-br[,6]), xlab="Estimated  Pr (depression)", ylab="Average residual", type="n", main="Binned residual plot", mgp=c(2,.5,0))
abline (0,0, col="gray", lwd=.5)
lines (br[,1], br[,6], col="gray", lwd=.5)
lines (br[,1], -br[,6], col="gray", lwd=.5)
points (br[,1], br[,2], pch=19, cex=.5)
```

The code below expresses the beta coefficients in terms of odds ratios. First we create a function that takes in the estimate and its standard error, and returns a 95% confidence interval. Given the beta coefficient for ADD, the 95% CI for the odds ration is [2.12, 13.48], which we can interpret as: given the presence of ADD (value=2), the ratio of the probabilities of p(depression=1) / p(depression=0) is within that range.

```{r}
beta_to_odds <- function(estimate, standard_error){
  beta_lower <- estimate - (1.96) * standard_error
  beta_upper <- estimate + (1.96) * standard_error
  
  odds_CI <- c(exp(beta_lower), exp(beta_upper))
  return (odds_CI)
}

ADD_est <- summary_logistic$coefficients[2,1]
ADD_se <- summary_logistic$coefficients[2,2]

print(beta_to_odds(ADD_est, ADD_se))
```

(a) (20 points) Use the forward step procedure to detect the important covariates. Then, only for estimates that are greater than 0, draw with ggplot a plot similar to Figure 1. So in the x-axis, you should have each beta (beta1, beta2, etc.). In the y-axis, the estimate greater than 0 with the correspondent standard error. Be careful this plot is taken from another data set, so do not expect similar results. Take special care of the legend and the label. What can you say about this plot?

```{r}
fit_start <- glm(depression ~ 1, data=df_pred, family=binomial(link="logit"))
fit_end <- glm(depression ~ ., data=df_pred, family=binomial(link="logit"))
forward_logistic <- stepAIC(fit_start, direction="forward", scope=list(upper=fit_end, lower=fit_start))
summary_forward_logistic <- summary(forward_logistic)
```

```{r}
forward_coefficients <- summary_forward_logistic$coefficients
forward_mean <- forward_coefficients[2:dim(forward_coefficients)[1],1]
forward_errors <- forward_coefficients[2:dim(forward_coefficients)[1],2]
forward_names <- rownames(forward_coefficients)[2:dim(forward_coefficients)[1]]

df_forward <- data.frame(forward_names, forward_mean, forward_errors)
colnames(df_forward) <- c('names', 'mean', 'error')

df_forward <- filter(df_forward, df_forward$mean > 0)

ggplot(df_forward, aes(x=names, y=mean)) + 
  geom_errorbar(width=.1, aes(ymin=mean - (1.96*error), ymax=mean + (1.96*error))) +
  labs(y='beta coefficient estimate', x='beta coefficient names',
  title='Positive Beta Coefficient Estimates from forward model of p(depression)')
```

```{r, out.width="0.5\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Estimate"), echo=FALSE}
knitr::include_graphics("fig1.png")
```

(d) (20 points) Perform a bootstrap of 1000 samples for beta 1 (ADD or *p6b10*), beta 2 (sleep or *p6b55*), and beta 3 (attention at school or *k6b21a*) with a model that contains all the coefficients obtained in the forward procedure in point c. Plot these three bootstrapped beta coefficients that you have obtained with a boxplot in the ggplot (similar to Figure 2). (make sure not to use the default colors but rather choose your own). What can you say about these three distributions obtained?

```{r}
fc <- function(d, i){
  d2 <- d[i,]
  boot_fit <- glm()
  
  return(boot_fit[['coefficients']])
}

earn_boot <- boot(drop_na(df), fc, R=500)
```

```{r, out.width="0.6\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Boxplot"), echo=FALSE}
knitr::include_graphics("Fig2.pdf")
```

(e) (20 points) Perform the Lasso method for the full model. Choose $\lambda$ with the cross-validation. Then perform the lasso with the best $\lambda$ obtained. Plot the results in ggplot. Describe the results you obtained. Are the coefficients obtained with the lasso procedure similar to the coefficients obtained with the forward procedure? Explain!

```{r}
lasso_mod = glmnet(x_train, 
                   y_train, 
                   alpha = 1, 
                   lambda = grid)

plot(lasso_mod)

##CV 
set.seed(1)
# Fit lasso model on training data
cv.out = cv.glmnet(x_train, y_train, alpha = 1) 

# Select lambda that minimizes training MSE
bestlam = cv.out$lambda.min 

# Fit lasso model on full dataset
out = glmnet(x, y, alpha = 1, lambda = grid) 

# Display coefficients using lambda chosen by CV
lasso_coef = predict(out, type = "coefficients", s = bestlam)[1:20,] 
lasso_coef
lasso_mod <-  glmnet(x, y, alpha = 1)
```
