---
title: "Assignment 2"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**NAME: Nicholas Tolley**\
**DUE DATE: February 28th, 6pm**

## Problem 1 (100 pts)

In the earnings dataset you can find salary (*earn*) and some socio-demographic characteristics of each subject, including variables such as *height*, *weight*, gender (*male*), *ethnicity*, *education*, mother's (*mother_education*) and father's education (*father_education*), *walk* (e.g. walking time), *exercise*, if they smoke or not (*smokenow*), *tense*, *angry* and *age*.

The dataset can be found in Canvas in the Data folder (file name: earnings.csv):

(a) (10 points) Subset the data and consider only the variables: *education*, *mother_education*, *father_education*, *walk*, *exercise*, *tense*, *angry*, *weight*, *height*. Check the correlation by performing a figure similar to Figure 1 below (make sure not to use the default colours but rather choose your own). Take special care to the labels and legend. What can you say about the results? What would you expect from a linear regression model (hint: there are some variables to be excluded/included in the model)? Perform a test statistic for the correlation between earn and education, write the hypothesis test and the results you will obtain.

```{r}
library(ggplot2)
library(boot)
library(tidyverse)
library(GGally)
library(tidyr)
library(MASS)
library(caret)

df <- read.csv('earnings.csv')
```

The code below stores a subset of the dataset with the columns indicated above. Since we are calculating the correlation between columns, rows with missing values in any column are removed.

```{r}
subset_cols <- c('education', 'mother_education', 'father_education',
                 'walk', 'exercise', 'tense', 'angry', 'weight', 'height')
label_names <- c('Education', 'Mot._Education', 'Fat._Education', 'Walk', 'Exercise', 'Tense',
                 'Angry', 'Weight', 'Height')
df_subset <- subset(df, select=subset_cols)
df_subset <- drop_na(df_subset)
colnames(df_subset) <- label_names
```

Next we can calculate the correlation between the columns contained in the subset, and visualize the result as a heatmap.

```{r}
corr_matrix <- cor(df_subset)
ggcorr(df_subset, label=TRUE,  low="steelblue", mid="white", high="darkred")
```

As we can see there are several variables that exhibit a near perfect correlation with one another. The most highly correlated columns include:

education \<-\> mother_education

education \<-\> father_education

father_education \<-\> mother_education

tense \<-\> angry

If we were to build a linear regression model, we would need to remove 2 of the education variables, and either the tense or angry variable. This is because the dataset exhibits what is known as multicollinearity, in other words there is redundant information in the columns. If we were to try and create a linear model on the full dataset, there would not be a unique combination of regression ("beta") coefficients that minimize the residual error. For example, the same coefficient could be assigned tot he angry or tense columns.

The code below calculates the correlation coefficient, and associated p-value, between the earn and education columns. The p-value refers to the probability of the null hypothesis that these two variables are uncorrelated (correlation=0).

```{r}
cor.test(df$earn, df$education)
```

We can see that despite the correlation being relatively low (cor=0.3), the result is highly significant with a p-value \< 2.2e-16. We can therefore reject the null hypothesis that the "earn" and "education" variables are uncorrelated with one another.

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Correlation"), echo=FALSE}
knitr::include_graphics("ggcor.pdf")
```

(b) (10 points) Perform a linear regression model using the variable *earn* as the dependent variable and years of eductaion *education* as the independent variable. What can you say about this covariate? Is it significant? Write down the hypothesis test. Plot the linear regression you have obtained in ggplot by using a subset of the data. This subset is obtained by restricting the variable *earn* to be less than 2e+05 (similar to Figure 2 below)

The code below creates a linear model that predicts "earn" by the education covariate. From the linear model there are three potential hypothesis tests with the following null hypotheses:

-   The intercept of the linear model is zero

-   The beta coefficient for education of the linear model is zero

-   The model has the same explanatory power (residual error) as constant line set to the mean of the data

```{r}
earn_fit <- lm(df$earn ~ df$education)
summary(earn_fit)
```

As we can see from the from the summary of the model output above, both the intercept and the slope are highly significant indicating that the *earn* variable is predicted well by the *education* variable.

Next using the subset of the with *earn* \< 2e+05, we can visualize how well our linear model explains *earn* using just the *education* covariate

```{r}
earn_subset <- subset(df, df$earn < 2e5)

ggplot(earn_subset, aes(x=education, y=earn)) + geom_point(na.rm=TRUE) +   geom_smooth(method='lm', na.rm=TRUE) + labs(x='Education', y='Earnings', title='Fitted linear model') + theme_bw() 

```

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Linear Regression"), echo=FALSE}
knitr::include_graphics("fig2.pdf")
```

(c) (20 points) Draw the qqplot by using the library ggplot for the model obtained in point b. Then perform the qqplot (using the library ggplot) for the two different groups of sex (similar to Figure 3 below). Take special care to the legend and the label. What can you say about this plot?

The code below creates a QQ plot on the residuals of the model above.

As we can see the residuals are in fact not normally distributed. Instead we see that there is a rightward skew such that the mode of the distribution is negative, but a larger proportion of the probability mass is spread over positive residuals

```{r}
model_residuals <- resid(earn_fit)
p <- seq(0.05, 0.95, 0.01)
sample_quantiles <- quantile(model_residuals, p)
theoretical_quantiles <- qnorm(p, mean=mean(model_residuals), sd=sd(model_residuals))
qplot(theoretical_quantiles, sample_quantiles) + geom_abline() + labs(title='Residuals Q-Q Plot', y='Residuals', x='Theoretical') + theme_bw()
```

Next we can visualize the QQ plots of the earn variable (not the residuals) for males and females separately. To allow for direct comparison the shows the quantiles with respect to a standard normal distribution. The first observation we can make is that neither of these subgroups have normally distributed earnings. Instead there is a strong positive skew such that the majority of males and females make a relatively small amount of money, but a smaller proportion with high earnings are spread out.

```{r}
earn_quantiles_male <- quantile(subset(df, df$male==1)$earn, p, na.rm=TRUE)
earn_quantiles_female <- quantile(subset(df, df$male==0)$earn, p, na.rm=TRUE)

theoretical_quantiles_male <- qnorm(p, mean=0, sd=1)
theoretical_quantiles_female <- qnorm(p, mean=0, sd=1)

earn_quantiles <- c(earn_quantiles_male, earn_quantiles_female)
theoretical_quantiles <- c(theoretical_quantiles_male, theoretical_quantiles_female)
earn_sex <- c(rep('Male', each=length(earn_quantiles_male)),
              rep('Female', each=length(earn_quantiles_female)))

earn_sex_df <- data.frame(earn_quantiles, theoretical_quantiles, earn_sex)

ggplot(earn_sex_df, aes(x=theoretical_quantiles, y=earn_quantiles, color=earn_sex)) + 
  geom_point() + labs(title='Earnings for different groups of sex', y='Sample', x='Theoretical') +
  theme_bw()
```

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("QQplot for different groups"), echo=FALSE}
knitr::include_graphics("fig3.pdf")
```

(d) (20 points) Perform in R the backward and forward procedure to select the covariates, remember to remove the rows with missing values. Did you obtain the same or different results from the two different procedures, please explain. Which procedure would you prefer? Comment what you discovered and the theoretical implications. Just for the backward solution compute the RSS and show the trend of RSS for beta1 in a plot by using ggplot in R (similar to Figure 4). (Hint: For RSS plot, set the range of x-axis to be [0,1000]).

The code below performs the backward procedure to select the best fit model. The procedure starts with a model that contains all covariates, and successively removes variables that have the lowest predictive ability.

```{r}
fit1 <- lm(earn~., data=drop_na(df))
fit2 <- lm(earn~1, data=drop_na(df))
mod_backward <- stepAIC(fit1, direction="backward", scope=list(upper=fit1, lower=fit2))
summary(mod_backward)
```

Likewise, we can perform the forward pass by starting with a minimal model that predicts a flat line, and successively add the covariates which individually have the highest predictive ability (lowest RSS when they are used as the solve covariate).

```{r}
mod_forward <- stepAIC(fit2, direction="forward", scope=list(upper=fit1, lower=fit2))
summary(mod_forward)
```

From the results above we can see that the forward and backward passes actually arrive at the same final model of 'earn \~ height + male + education + tense + age'. Since the both approaches pick the optimal beta coefficients for these terms, the models are in fact identical.

To find the RSS associated with changing the beta 1 coefficient, we will first create a function that manually performs the matrix multiplication using the weights arrived at from the backward pass, and then updates the coefficient based on our input. With this function we can simply pass in the true beta coefficient for the height variable and see that the resulting RSS is 6.46e+11

```{r}
beta_update <- function(new_val, mod, d){
  beta_coef <- mod[["coefficients"]]
  beta_coef[2] <- new_val
  mod_d <- subset(d, select=names(mod[["coefficients"]][c(2,3,4,5,6)]))
  mod_d <- data.frame(intercept=rep(1, nrow(mod_d)), mod_d)
  pred <- as.matrix(mod_d) %*% beta_coef
  
  res <- sum((d$earn - pred)^2)
  return(res)
}

beta_update(mod_backward[["coefficients"]][2], mod_backward, drop_na(df))

```

Using this function in a for loop we can plot the RSS as a function of the beta 1 coefficient and observe that the minima occurs exactly at the beta coefficient used in the true model (red line).

```{r}
rss_out <- rep(0, 1000)
for (i in 1:1000){
  rss_out[i] <- beta_update(i, mod_backward, drop_na(df))
}

rss_df <- data.frame(RSS=rss_out, beta1=c(1:1000))

ggplot(data=rss_df, aes(x=beta1, y=rss_out)) + geom_point() + 
  labs(title="RSS with altered height coefficient", y="RSS") + 
  geom_vline(xintercept=mod_backward[["coefficients"]][2], color="red")
```

```{r, out.width="0.8\\linewidth", include=TRUE, fig.align="center", fig.cap=c("RSS for the backward procedure"), echo=FALSE}
knitr::include_graphics("fig4.pdf")
```

(e) (20 points) Perform a bootstrap of 500 samples for beta 1 (*height*), beta 2 (*male*), and beta 3 (*education*) for the coefficient obtained in the backward procedure in point d. Plot the beta coefficients that you have obtained with histograms with ggplot (similar to Figure 5). Remember to use the data without missing values.

The code below creates a function that fits a linear model to a subset of the earnings dataset, and returns the resulting beta coefficients. Combining this with bootstrap function, we can aggregate the beta coefficients for randoms sets of 500.

```{r}
fc <- function(d, i){
  d2 <- d[i,]
  boot_fit <- lm(earn ~ height + male + education + tense + age, data=d2)
  
  return(boot_fit[['coefficients']])
}

earn_boot <- boot(drop_na(df), fc, R=500)
```

Next we can plot the beta coefficients from the bootstrap samples as a histogram and plot them against the true beta coefficients derived from a linear model on the full earnings dataset.

```{r}
earn_boot_coef <- data.frame(earn_boot[["t"]])
colnames(earn_boot_coef) <- names(earn_boot[["t0"]])

earn_boot_coef <- subset(earn_boot_coef, select=c('height', 'male', 'education'))
earn_boot_coef <- gather(earn_boot_coef)
earn_boot_vline <- data.frame(key=c('height', 'male', 'education'), 
                              value=c(earn_boot[["t0"]][["height"]], 
                                      earn_boot[["t0"]][["male"]], 
                                      earn_boot[["t0"]][["education"]]))

ggplot(earn_boot_coef, aes(x=value)) + geom_histogram(bins=50) + 
  labs(x='Beta Coefficient', y='Count') +
  geom_vline(data=earn_boot_vline, aes(xintercept=value), color="red") +
  facet_wrap(~ key)
  theme_bw()
```

```{r, out.width="1.12\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Bootstrap Results"), echo=FALSE}
knitr::include_graphics("bootstrap.pdf")
```

(f) (20 points) Compute the LOO and K-fold cross validation and write the results. Compute the mean square error for both the LOO and the K-fold cross validation. Then plot the prediction against the true value for LOO, using ggplot. Describe the results. Remember to use the data without missing values.

The code below performs leave-one-out cross validation using the model arrived at from the forward and backward passes. The RMSE calculated using this procedure is 21,289.3, or an MSE of 453,234,294.

```{r}
train.control <- trainControl(method = "LOOCV")
model_loocv <- train(earn~height + male + education + tense + age, data=drop_na(df),
               method="lm", trControl=train.control)
print(model_loocv)
summary(model_loocv)
```

The code below performs the same steps for 10 fold cross validation. We see that the RMSE from this procedure is 20,572.79, or an MSE of 423,239,688

```{r}
train.control <- trainControl(method = "cv", number=10)
model_cv <- train(earn~height + male + education + tense + age, data=drop_na(df),
               method="lm", trControl=train.control)
print(model_cv)
summary(model_cv)
```

```{r}
model_summ_cv <-summary(model_cv[["finalModel"]])
mean(model_summ_cv$residuals^2)
```

Comparing the results of the two cross validation procedures, we see that the estimated MSE is slightly smaller using 10 fold cross validation. As expected from the MSE, we additionally see the estimated R squared from 10 fold CV is higher for LOOCV (0.24 vs 0.2). Despite these minor differences it can be concluded that the procedures produce largely similar results for the efficacy this model for predicting earnings.

The code below plots the true vs the predicted earnings using the model from the LOOCV procedure. As we can see the predictions are concentrated around the diagonal which indicates perfect accuracy. However there are several outliers where the true earnings for certain individuals are predicted to be much lower than reality. Additionally we see that some individuals are predicted to have negative earnings which is clearly unrealistic since the minimum earnings is zero.

```{r}
pred <- predict(model_loocv, newdata = drop_na(df))
cv_df <- data.frame(true_earn=drop_na(df)$earn, pred_earn=pred)
ggplot(cv_df, aes(x=true_earn, y=pred_earn)) + geom_point() +
  labs(title='Linear Model True Earnings vs Predicted Earnings', x='True', y='Predicted') + theme_bw() + geom_abline()
```
