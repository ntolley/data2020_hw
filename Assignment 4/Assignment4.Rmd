---
title: "Assignment 4"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**NAME: Nicholas Tolley**\
**DUE DATE: April 4th, 6pm**

## Problem 1 (100 pts)

In the folder Assignment 4, you will find the data set called data-final.csv. This data set is from the Five Personality Data Set, and it collects on-line personality test (take a look to the codebook.txt in the folder Assignment 4).

(a) (40 points) Consider the first 50 variables of this data set (this should correspond to the codebook.txt variables). Perform the Principal Component (PC) analysis after having scaled the data. How many components will you retain based on the total variance explained by each component? Plot a bar plot (in ggplot) showing the proportion of variance explained by each PC (consider just the first 10 PC). Then, plot the PC that you have chosen in an heatmap, choose your own colour in three different tonality (where one should be white). How can you interpret this plot and the PC? Is there any link with the name of the data set \`\`the five big personalities''?

```{r}
rm(list=ls())
library(dplyr)
library(tidyr)
library(boot)
library(ggplot2)
library(RColorBrewer)

df <- read.csv('data-final.csv', sep='\t', header=TRUE)
df <- df[1:1000,1:50]
```

The code below loops through each column in the dataframe and scales the data.

```{r}
df_scaled <- df
for (i in 1:ncol(df_scaled)){
  df_scaled[,i] <- scale(as.numeric(df_scaled[,i]))
}
```

Next we perform PCA on the scaled data and save the results. We can calculate the variance explained by using the fact that the eigen values are equal to the variance, and therefore calculate the ratio of each eigen value to the sum of all eigenvalues. Using a variance explained cutoff of 90%, we can see that it will be necessary to retain the first 35 components.

```{r}
pca_res <- prcomp(df_scaled)
eigs <- pca_res$sdev^2
var_explained <- eigs / sum(eigs)
sum(cumsum(var_explained) < 0.9)
```

```{r}
var_exp_df <- data.frame(pca_idx=1:10, var_exp=var_explained[1:10])

ggplot(var_exp_df, aes(x=pca_idx,y=var_exp)) + geom_bar(stat="identity") + 
  labs(x="Principal Component", y="Variance Explained")
```

Using the PC's with the variance explained cut off above, we can plot their loadings as a heatmap as shown below. Inspecting the dendrogram that represents the similarities between loadings, we can see that there are generally 5 major branches which presumably correspond to the big 5 personality types.

```{r}
heatmap(pca_res$rotation[,1:35], col=brewer.pal(11,"RdBu"))
```

```{r}

```

(a) (40 points) Perform a factor analysis model with 5 factors with no rotation. How is the total variance explained from the model? Now perform the factor analysis model with 5 factors and with the varimax rotation (remember to not scale the data). Will you keep the model with 5 common factors or will you add another one? Explain why. Plot in an heatmap the matrix of factor loadings matrix (similar to Figure 1) Again choose your own colour by considering three different tonality. Interpretation: Now interpret the factors. Explain what each factor represents and give a name to each factor based on its high loadings.

First we will convert the dataframe to numeric columns so that it can be handled directly by the factanal function.

```{r}
df_numeric <- df
for (i in 1:ncol(df_scaled)){
  df_numeric[,i] <- as.numeric(df_numeric[,i])
}

```

Below we perform factor analysis with no rotation. When including just 5 factors, we see that the total variance explained from the model is fairly low at 38.4% (***CHANGE THIS***)

```{r}
fact_no_rot <- factanal(x=df_numeric, factors=5, rotation="none")
fact_no_rot
```

Next we perform the same analysis with varimax rotation. We can observe that the analysis results are almost entirely identical, except for the loadings on each individual factor. In both analyses the results at the bottom indicate a statistical test where the null hypothesis is that 5 factors is sufficient to explain the observed data. Since the p-value is very low, we reject this null hypothesis, indicating that we should include more factors in our model.

```{r}
fact_varimax <- factanal(x=df_numeric, factors=5, rotation="varimax")
print(fact_varimax)
```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Estimate"), echo=FALSE}
knitr::include_graphics("HeatmapFA.pdf")
```

(c) (20 points) Perform a bootstrap of 50 samples. For each of the bootstrapped sample save the proportion of variance explained by each factor (consider just the first five factors). Plot the proportion of variance explained by each of the five factors with a boxplot in the ggplot and then perform the histogram for each proportion. What can you say about these five distributions obtained? If we bootstrap the loadings we will obtain something no sense in a statistical framework. Explain why.

```{r}
fc <- function(df, i){
  df_boot <- df[i,]
  factanal_res <- factanal(x=df_numeric, factors=5, rotation="varimax")
  loadings_res <- loadings(factanal_res)
  prop_var <- colSums(loadings_res^2) / nrow(loadings_res)
  return(prop_var)
}

factanal_boot <- boot(df_numeric, fc, R=50)
```

```{r}
df_boot_beta <- data.frame(depression_boot[["t"]][,1:length(forward_names) + 1])
colnames(df_boot_beta) <- forward_names

boot_cols <- c('ADD', 'sleep_trouble', 'school_attention_problem')
df_boot_beta <-df_boot_beta[, boot_cols]
colnames(df_boot_beta) <- c('Beta1', 'Beta2', 'Beta3')

df_boot_beta <- pivot_longer(df_boot_beta, cols=colnames(df_boot_beta), names_to = "beta", values_to = "coef")

ggplot(df_boot_beta, aes(x=beta, y=coef, fill=beta)) + geom_boxplot() + 
  scale_fill_viridis(discrete = TRUE, alpha=0.6) +
  labs(x='Beta', y='Estimate', title='Bootstrap (r=1000) beta estimates for p(depression) glm') + theme_bw()
```
