---
title: "Assignment 4"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**NAME: Nicholas Tolley**\
**DUE DATE: April 4th, 6pm**

## Problem 1 (100 pts)

In the folder Assignment 4, you will find the data set called data-final.csv. This data set is from the Five Personality Data Set, and it collects on-line personality test (take a look to the codebook.txt in the folder Assignment 4).

(a) (40 points) Consider the first 50 variables of this data set (this should correspond to the codebook.txt variables). Perform the Principal Component (PC) analysis after having scaled the data. How many components will you retain based on the total variance explained by each component? Plot a bar plot (in ggplot) showing the proportion of variance explained by each PC (consider just the first 10 PC). Then, plot the PC that you have chosen in an heatmap, choose your own colour in three different tonality (where one should be white). How can you interpret this plot and the PC? Is there any link with the name of the data set \`\`the five big personalities''?

```{r}
rm(list=ls())
library(dplyr)
library(tidyr)
library(boot)
library(ggplot2)
library(RColorBrewer)

df <- read.csv('data-final.csv', sep='\t', header=TRUE)
df <- df[,1:50]
```

The code below loops through each column in the dataframe and scales the data.

```{r}
df_scaled <- df
for (i in 1:ncol(df_scaled)){
  df_scaled[,i] <- scale(as.numeric(df_scaled[,i]))
}
```

Next we perform PCA on the scaled data and save the results. We can calculate the variance explained by using the fact that the eigen values are equal to the variance, and therefore calculate the ratio of each eigen value to the sum of all eigenvalues. Using a variance explained cutoff of 90%, we can see that it will be necessary to retain the first 35 components.

```{r}
pca_res <- prcomp(df_scaled)
eigs <- pca_res$sdev^2
var_explained <- eigs / sum(eigs)
sum(cumsum(var_explained) < 0.9)
```

```{r}
var_exp_df <- data.frame(pca_idx=1:10, var_exp=var_explained[1:10])

ggplot(var_exp_df, aes(x=pca_idx,y=var_exp)) + geom_bar(stat="identity") + 
  labs(x="Principal Component", y="Variance Explained")
```

Using the PC's with the variance explained cut off above, we can plot their loadings as a heatmap as shown below. Inspecting the dendrogram that represents the similarities between loadings, we can see that there are generally 5 major branches which presumably correspond to the big 5 personality types.

```{r}
heatmap(pca_res$rotation[,1:35], col=brewer.pal(11,"RdBu"))
```

```{r}

```

(b) (40 points) Perform a factor analysis model with 5 factors with no rotation. How is the total variance explained from the model? Now perform the factor analysis model with 5 factors and with the varimax rotation (remember to not scale the data). Will you keep the model with 5 common factors or will you add another one? Explain why. Plot in an heatmap the matrix of factor loadings matrix (similar to Figure 1) Again choose your own colour by considering three different tonality. Interpretation: Now interpret the factors. Explain what each factor represents and give a name to each factor based on its high loadings.

First we will convert the dataframe to numeric columns so that it can be handled directly by the factanal function.

```{r}
df_numeric <- df
for (i in 1:ncol(df_scaled)){
  df_numeric[,i] <- as.numeric(df_numeric[,i])
}

```

Below we perform factor analysis with no rotation. When including just 5 factors, we see that the total variance explained from the model is fairly low at 39%

```{r}
fact_no_rot <- factanal(x=df_numeric, factors=5, rotation="none")
fact_no_rot
```

Next we perform the same analysis with varimax rotation. We can observe that the analysis results are almost entirely identical, except for the loadings on each individual factor. In both analyses the results at the bottom indicate a statistical test where the null hypothesis is that 5 factors is sufficient to explain the observed data. Since the p-value is very low (approximately zero), we reject this null hypothesis, indicating that we should include more factors in our model.

```{r}
fact_varimax <- factanal(x=df_numeric, factors=5, rotation="varimax")
print(fact_varimax)
```

Finally we can inspect the loadings using a heat map. As shown below, the loadings on each factor exhibit a clear pattern where variables of the same 3 letter prefix exhibit large negative/positive (dark blue/red) values, with the remaining values close to zero. From the code book, we can see that these actually correspond to the "Big Five Factor Markers", where EXT=extraversion, EST=emotional stability, AGR=agreeableness, CSN=conscientiousness, and OPN=openess. This indicates that responses within these categories strongly co-vary with each other. For example, if someone rates EXT1 with a disagree response (disagree=1, question: I am the life of the party), it is strongly likely that they will respond with an agree response to EXT2 (agree=5, question:I don't talk a lot).

```{r}
heatmap(fact_varimax$loadings, col=brewer.pal(11,"RdBu"),  Rowv = NA, Colv = NA)
```

```{r, out.width="0.9\\linewidth", include=TRUE, fig.align="center", fig.cap=c("Estimate"), echo=FALSE}
knitr::include_graphics("HeatmapFA.pdf")
```

(c) (20 points) Perform a bootstrap of 50 samples. For each of the bootstrapped sample save the proportion of variance explained by each factor (consider just the first five factors). Plot the proportion of variance explained by each of the five factors with a boxplot in the ggplot and then perform the histogram for each proportion. What can you say about these five distributions obtained? If we bootstrap the loadings we will obtain something no sense in a statistical framework. Explain why.

The code below calculates the proportion of variance explained by using the fact that the sum of the loadings squared for a given factor, divided by the total number of variables, is equivalent to the proportion of variance explained. Plugging this into a function that performs factor analysis for a subset of dataset, we can perform the desired bootstrap to obtain estimates for the proportion of variance explained.

```{r}
fc <- function(df, i){
  df_boot <- df[i,]
  factanal_res <- factanal(x=df_boot, factors=5, rotation="varimax")
  loadings_res <- loadings(factanal_res)
  prop_var <- colSums(loadings_res^2) / nrow(loadings_res)
  return(prop_var)
}

factanal_boot <- boot(df_numeric, fc, R=50)
```

By plotting the bootstrap estimates of the proportion of variance explained on a box plot, we can clearly see that the factors are ordered by proportion of variance explained. The first 2 factors corresponding to extraversion and emotional stability seem to exhibit that highest proportion of variance explained (\~10%), whereas the remaining are \~6-7%.

```{r}
factanal_boot_df <- data.frame(factanal_boot[["t"]])
factanal_boot_df <- pivot_longer(factanal_boot_df, cols=colnames(factanal_boot_df), names_to = "factor", values_to = "propvar")

ggplot(factanal_boot_df, aes(x=factor, y=propvar, fill=factor)) + geom_boxplot() + 
  labs(x='Factor', y='Proportion Variance', title='Bootstrap (r=50) proportion variance explaind') + theme_bw()
```

Plotting the same data above in the form of histograms, we can more clearly see the shape of the distributions. Same as above, the mean of the distributions shift left in accordance with a lower proportion of variance explained. In general however we see that the distributions are exceptionally narrow with now outliers, indicating that each bootstrap sample estimate is largely similar.

```{r}
ggplot(factanal_boot_df, aes(x=propvar)) + geom_histogram(bins=20) + 
  labs(x='Proportion Variance Explained', y='Count') +
  facet_wrap(~ factor)
  theme_bw()
```

Finally, the reason it is not statistically meaningful to bootstrap the loadings is because the loadings are not uniquely determined. Ultimately the goal of factor analysis is to model the covariance structure of the observed data. However, this constraint alone is insufficient for uniqueness. While this issue is partially mitigated by identifying "rotations" that add extra constraints of factor loadings (varimax for example), the optimally rotated loadings across bootstrap samples of the dataset may still be very different from one sample to the next.
